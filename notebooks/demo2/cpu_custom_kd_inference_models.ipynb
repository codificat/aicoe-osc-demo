{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee11b421",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inference with teacher-student Models \n",
    "\n",
    "Custom knowledge distillation is a technique in deep learning that involves training a smaller, simpler model (the \"student\") to mimic the behavior of a larger, more complex model (the \"teacher\"). This process is intended to improve the efficiency of deep learning models, by compressing the knowledge of the larger model into a smaller one, without sacrificing too much accuracy. In this notebook, we explore the capabilities of DeepSparse and SparseZoo in optimizing and executing custom knowledge distillation models on the CPU.\n",
    "\n",
    "Specifically, we are conducting inference tests on custom knowledge distillation models using DeepSparse and SparseZoo, while evaluating the impact of the number of CPU cores. Our experiments focus on calculating the inference time for three pre-trained models: '12layer_pruned80-none', '12layer_pruned90-none', and 'teacher'. We will be testing these models using 4 and 8 cores of the CPU, which will allow us to investigate the effect of core count on the inference time of these models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc1eb0c2",
   "metadata": {
    "papermill": {
     "duration": 3.30074,
     "end_time": "2022-10-07T19:33:18.885511",
     "exception": false,
     "start_time": "2022-10-07T19:33:15.584771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from dotenv import load_dotenv\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from src.data.s3_communication import S3Communication, S3FileType\n",
    "from src.components.utils.kpi_mapping import get_kpi_mapping_category\n",
    "import json\n",
    "import time\n",
    "import config\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "from torch import cuda\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad4b2028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "dotenv_dir = os.environ.get(\n",
    "    \"CREDENTIAL_DOTENV_DIR\", os.environ.get(\"PWD\", \"/opt/app-root/src\")\n",
    ")\n",
    "dotenv_path = pathlib.Path(dotenv_dir) / \"credentials.env\"\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path=dotenv_path, override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d61eb734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init s3 connector\n",
    "s3c = S3Communication(\n",
    "    s3_endpoint_url=os.getenv(\"S3_ENDPOINT\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "    s3_bucket=os.getenv(\"S3_BUCKET\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd72537",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b55e007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(data_df, batch_size=32):\n",
    "    encoded_dataset = list()\n",
    "    batch = list()\n",
    "    for df, row in data_df.iterrows():\n",
    "        if len(batch) < batch_size:\n",
    "            batch.append([row['question'], row['sentence']])\n",
    "        else:\n",
    "            encoded_dataset.append(tokenizer(batch,\n",
    "                                             truncation=True,\n",
    "                                             return_tensors='pt',\n",
    "                                             padding=True))\n",
    "            batch = [[row['question'], row['sentence']]]\n",
    "    if batch:\n",
    "        encoded_dataset.append(tokenizer(batch,\n",
    "                                         truncation=True,\n",
    "                                         return_tensors='pt',\n",
    "                                         padding=True))\n",
    "    return encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da4b1a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_data(pdf_name, pdf_path):\n",
    "    pdf_content = read_text_from_json(file_path)\n",
    "    text_data = []\n",
    "    # Build all possible combinations of paragraphs and  questions\n",
    "    # Keep track of page number which the text is extracted from and\n",
    "    # the pdf it belongs to.\n",
    "    for kpi_question in questions:\n",
    "        text_data.extend([{\n",
    "            \"page\": page_num,\n",
    "            \"pdf_name\": pdf_name,\n",
    "            \"question\": kpi_question,\n",
    "            \"sentence\": paragraph}\n",
    "            for page_num, page_content in pdf_content.items()\n",
    "            for paragraph in page_content])\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e447928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(encoded_dataset):\n",
    "    outputs = list()\n",
    "    for batch in encoded_dataset:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        with torch.no_grad():\n",
    "            outs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            outputs.extend(outs.logits.argmax(axis=1).tolist())\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46ce22e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_from_json(file):\n",
    "    \"\"\"Read text from json.\"\"\"\n",
    "\n",
    "    with open(file) as f:\n",
    "        text = json.load(f)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db64acf",
   "metadata": {},
   "source": [
    "## Retrieve the test dataset and the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75a79818",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3c.download_files_in_prefix_to_dir(\n",
    "    config.BASE_TRAIN_TEST_DATASET_S3_PREFIX,\n",
    "    config.BASE_PROCESSED_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "829a635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = str(config.BASE_PROCESSED_DATA)+'/rel_test_split.csv'\n",
    "test_data = pd.read_csv(test_data_path, index_col=0)\n",
    "test_data.rename(columns={'text': 'question', 'text_b':'sentence'}, inplace=True)\n",
    "\n",
    "train_data_path = str(config.BASE_PROCESSED_DATA)+'/rel_train_split.csv'\n",
    "train_data = pd.read_csv(train_data_path, index_col=0)\n",
    "train_data.rename(columns={'text': 'question', 'text_b':'sentence'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32210f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "trds = Dataset.from_pandas(train_data)\n",
    "teds = Dataset.from_pandas(test_data.drop('label', axis=1))\n",
    "\n",
    "climate_dataset = DatasetDict()\n",
    "\n",
    "climate_dataset['train'] = trds\n",
    "climate_dataset['test'] = teds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "790cd0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'question', 'sentence', '__index_level_0__'],\n",
       "        num_rows: 2033\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'sentence', '__index_level_0__'],\n",
       "        num_rows: 509\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climate_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7f3622",
   "metadata": {},
   "source": [
    "PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddcfeddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/opt/app-root/src/data')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.DATA_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97cea1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARK_FOLDER = config.DATA_FOLDER\n",
    "if not os.path.exists(BENCHMARK_FOLDER):\n",
    "    BENCHMARK_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BENCHMARK_EXTRACTION_FOLDER = BENCHMARK_FOLDER / \"extraction\"\n",
    "if not os.path.exists(BENCHMARK_EXTRACTION_FOLDER):\n",
    "    pathlib.Path(BENCHMARK_EXTRACTION_FOLDER).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "517064c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpi_df = s3c.download_df_from_s3(\n",
    "    \"aicoe-osc-demo/kpi_mapping\",\n",
    "    \"kpi_mapping.csv\",\n",
    "    filetype=S3FileType.CSV,\n",
    "    header=0)\n",
    "\n",
    "kmc = get_kpi_mapping_category(kpi_df)\n",
    "questions = [q_text for q_id, (q_text, sect) in kmc[\"KPI_MAPPING_MODEL\"].items()\n",
    "             if len(set(sect).intersection({\"OG\", \"CM\", \"CU\"})) > 0\n",
    "             and \"TEXT\" in kmc[\"KPI_CATEGORY\"][q_id]]\n",
    "\n",
    "text_paths = sorted(BENCHMARK_EXTRACTION_FOLDER.rglob(\"*.json\"))\n",
    "all_text_path_dict = {os.path.splitext(os.path.basename(file_path))[0]:\n",
    "                      file_path for file_path in text_paths\n",
    "                      if \"table_meta\" not in str(file_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af60178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing 10 random pdfs\n",
    "all_text_path_dict = dict(random.sample(all_text_path_dict.items(), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc426109",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "metrics_df_list = []\n",
    "metrics_list = []\n",
    "metric_dfs = pd.DataFrame()\n",
    "num_pdfs = len(all_text_path_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79a6659f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LUKOIL_SUSTAINABILITY_REPORT_2018': PosixPath('/opt/app-root/src/data/extraction/LUKOIL_SUSTAINABILITY_REPORT_2018.json'),\n",
       " 'equinor-2019-annual-report-and-form-20f': PosixPath('/opt/app-root/src/data/extraction/equinor-2019-annual-report-and-form-20f.json'),\n",
       " '413750375_Avista Corp_2019-12-31': PosixPath('/opt/app-root/src/data/extraction/413750375_Avista Corp_2019-12-31.json'),\n",
       " '2018 Annual Report': PosixPath('/opt/app-root/src/data/extraction/2018 Annual Report.json'),\n",
       " 'Ervia-Annual-Report-2018': PosixPath('/opt/app-root/src/data/extraction/Ervia-Annual-Report-2018.json'),\n",
       " 'SaipemSustainability2018': PosixPath('/opt/app-root/src/data/extraction/SaipemSustainability2018.json'),\n",
       " 'RN_SR_2016_EN_2_ sustainabilitz 2016': PosixPath('/opt/app-root/src/data/extraction/RN_SR_2016_EN_2_ sustainabilitz 2016.json'),\n",
       " 'Eskom Holdings SOC Ltd Integrated Report 2019': PosixPath('/opt/app-root/src/data/extraction/Eskom Holdings SOC Ltd Integrated Report 2019.json'),\n",
       " 'Sustainability Report 2016_EN': PosixPath('/opt/app-root/src/data/extraction/Sustainability Report 2016_EN.json'),\n",
       " 'annual_report_2019_eng': PosixPath('/opt/app-root/src/data/extraction/annual_report_2019_eng.json')}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text_path_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca32a7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(num_pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd32ffe5-cd92-498f-b27d-d8b5f8353504",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_paths=['models/12layer_pruned80-none/',\n",
    "                   'models/12layer_pruned90-none/',\n",
    "                   'models/teacher/']\n",
    "\n",
    "model_names = ['12layer_pruned80-none',\n",
    "               '12layer_pruned90-none',\n",
    "               'teacher']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f53ad175-e12a-4cff-ab5a-64e835db28a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_paths=['models/teacher/']\n",
    "\n",
    "model_names = ['teacher']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "118ac1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop : 0\n",
      "Processing 1/10, LUKOIL_SUSTAINABILITY_REPORT_2018\n",
      "loop : 1\n",
      "Processing 2/10, equinor-2019-annual-report-and-form-20f\n",
      "loop : 2\n",
      "Processing 3/10, 413750375_Avista Corp_2019-12-31\n",
      "loop : 3\n",
      "Processing 4/10, 2018 Annual Report\n",
      "loop : 4\n",
      "Processing 5/10, Ervia-Annual-Report-2018\n",
      "loop : 5\n",
      "Processing 6/10, SaipemSustainability2018\n",
      "loop : 6\n",
      "Processing 7/10, RN_SR_2016_EN_2_ sustainabilitz 2016\n",
      "loop : 7\n",
      "Processing 8/10, Eskom Holdings SOC Ltd Integrated Report 2019\n",
      "loop : 8\n",
      "Processing 9/10, Sustainability Report 2016_EN\n",
      "loop : 9\n",
      "Processing 10/10, annual_report_2019_eng\n"
     ]
    }
   ],
   "source": [
    "metric_list = []\n",
    "for local_model_path, model_name in zip(local_model_paths,model_names):\n",
    "    for i, (pdf_name,file_path) in enumerate(all_text_path_dict.items()):\n",
    "        print(f\"loop : {i}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(local_model_path, use_fast=True)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(local_model_path).to(device)\n",
    "        encoded_dataset = create_batches(test_data)\n",
    "\n",
    "        print(f'Processing {i+1}/{len(all_text_path_dict)}, {pdf_name}')\n",
    "        data = gather_data(pdf_name, file_path)\n",
    "        num_data_points = len(data)\n",
    "        num_pages = data[len(data)-1]['page']\n",
    "        chunk_size = 1000\n",
    "        chunk_idx = 0\n",
    "        total_file_time = 0\n",
    "\n",
    "        predictions = list()\n",
    "        while chunk_idx * chunk_size < num_data_points:\n",
    "            chunk_start = time.time()\n",
    "            data_chunk = data[chunk_idx * chunk_size:(chunk_idx + 1) * chunk_size]\n",
    "            temp_df = pd.DataFrame(data_chunk).drop(['pdf_name', 'page'], axis=1)\n",
    "            encoded_dataset = create_batches(temp_df, batch_size=128)\n",
    "            predictions.extend(predict(encoded_dataset))\n",
    "            chunk_idx += 1\n",
    "            chunk_end = time.time()\n",
    "            total_file_time += (chunk_end - chunk_start)\n",
    "\n",
    "        time_per_data_point = total_file_time / num_data_points\n",
    "        data_points_per_sec = 1/time_per_data_point\n",
    "        model_size = os.path.getsize(local_model_path + 'pytorch_model.bin')/1000000\n",
    "\n",
    "        metric_list.append(\n",
    "            {'Model Name':model_name,\n",
    "             'Model Size(MB)': model_size,\n",
    "             'PDF Name':pdf_name,\n",
    "             'Number of Pages':int(num_pages),\n",
    "             'Number of Data Points':num_data_points,\n",
    "             'Total Inference Time':total_file_time,\n",
    "             'Time per data point':time_per_data_point,\n",
    "             'Data points per sec':data_points_per_sec})\n",
    "\n",
    "    file_to_save = pd.DataFrame(metric_list)\n",
    "    file_to_save.to_csv(f\"file_to_save_{model_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26b575aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dfs = pd.DataFrame(metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39993a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Model Size(MB)</th>\n",
       "      <th>PDF Name</th>\n",
       "      <th>Number of Pages</th>\n",
       "      <th>Number of Data Points</th>\n",
       "      <th>Total Inference Time</th>\n",
       "      <th>Time per data point</th>\n",
       "      <th>Data points per sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>teacher</td>\n",
       "      <td>438.011337</td>\n",
       "      <td>LUKOIL_SUSTAINABILITY_REPORT_2018</td>\n",
       "      <td>81</td>\n",
       "      <td>52392</td>\n",
       "      <td>201.269272</td>\n",
       "      <td>0.003842</td>\n",
       "      <td>260.307992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>teacher</td>\n",
       "      <td>438.011337</td>\n",
       "      <td>equinor-2019-annual-report-and-form-20f</td>\n",
       "      <td>317</td>\n",
       "      <td>82368</td>\n",
       "      <td>489.522515</td>\n",
       "      <td>0.005943</td>\n",
       "      <td>168.261924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>teacher</td>\n",
       "      <td>438.011337</td>\n",
       "      <td>413750375_Avista Corp_2019-12-31</td>\n",
       "      <td>79</td>\n",
       "      <td>81072</td>\n",
       "      <td>108.448538</td>\n",
       "      <td>0.001338</td>\n",
       "      <td>747.561948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>teacher</td>\n",
       "      <td>438.011337</td>\n",
       "      <td>2018 Annual Report</td>\n",
       "      <td>147</td>\n",
       "      <td>34152</td>\n",
       "      <td>205.439296</td>\n",
       "      <td>0.006015</td>\n",
       "      <td>166.238887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>teacher</td>\n",
       "      <td>438.011337</td>\n",
       "      <td>Ervia-Annual-Report-2018</td>\n",
       "      <td>155</td>\n",
       "      <td>41232</td>\n",
       "      <td>151.551620</td>\n",
       "      <td>0.003676</td>\n",
       "      <td>272.065717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model Name  Model Size(MB)                                 PDF Name  \\\n",
       "0    teacher      438.011337        LUKOIL_SUSTAINABILITY_REPORT_2018   \n",
       "1    teacher      438.011337  equinor-2019-annual-report-and-form-20f   \n",
       "2    teacher      438.011337         413750375_Avista Corp_2019-12-31   \n",
       "3    teacher      438.011337                       2018 Annual Report   \n",
       "4    teacher      438.011337                 Ervia-Annual-Report-2018   \n",
       "\n",
       "   Number of Pages  Number of Data Points  Total Inference Time  \\\n",
       "0               81                  52392            201.269272   \n",
       "1              317                  82368            489.522515   \n",
       "2               79                  81072            108.448538   \n",
       "3              147                  34152            205.439296   \n",
       "4              155                  41232            151.551620   \n",
       "\n",
       "   Time per data point  Data points per sec  \n",
       "0             0.003842           260.307992  \n",
       "1             0.005943           168.261924  \n",
       "2             0.001338           747.561948  \n",
       "3             0.006015           166.238887  \n",
       "4             0.003676           272.065717  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b60aad1-772f-428c-be04-08db8cf5c023",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv(\"file_to_save_cpu412layer_pruned80-none.csv\")\n",
    "df8 = pd.read_csv(\"file_to_save_cpu8obert_mnli_pruned90.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093c040c",
   "metadata": {},
   "source": [
    "**Model Name: 12layer_pruned80-none, Size: 438.02 MB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23de10b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv(\"file_to_save_cpu412layer_pruned80-none.csv\")\n",
    "df8 = pd.read_csv(\"file_to_save_cpu812layer_pruned80-none.csv\")\n",
    "df14 = df4[df4['Model Name']=='12layer_pruned80-none']\n",
    "df18 = df8[df8['Model Name']=='12layer_pruned80-none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30bc1762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5.000000\n",
       "mean     0.033795\n",
       "std      0.000038\n",
       "min      0.033733\n",
       "25%      0.033794\n",
       "50%      0.033804\n",
       "75%      0.033807\n",
       "max      0.033838\n",
       "Name: Time per data point, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average time per data point\n",
    "\n",
    "df14['Time per data point'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb5cb01",
   "metadata": {},
   "source": [
    "The average time per data point is 0.033795 seconds. A pdf with on average ~157 pages, and ~387 data points per page, will take ~34 mins, for 4 cores of CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "418326a7-4110-49ea-8392-c239757dc7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5.000000\n",
       "mean     0.016303\n",
       "std      0.000048\n",
       "min      0.016223\n",
       "25%      0.016296\n",
       "50%      0.016323\n",
       "75%      0.016333\n",
       "max      0.016338\n",
       "Name: Time per data point, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average time per data point\n",
    "\n",
    "df18['Time per data point'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8616f17-6529-4d44-84b2-24e148e88168",
   "metadata": {},
   "source": [
    "The average time per data point is 0.016303 seconds. A pdf with on average ~157 pages, and ~387 data points per page, will take ~16 mins, for 8 cores of CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db31b48",
   "metadata": {},
   "source": [
    "**Model Name: 12layer_pruned90-none, Size: 438.02 MB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e337f0b-8591-4b16-a44e-1bf79f6879d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df44 = pd.read_csv(\"file_to_save_cpu412layer_pruned90-none.csv\")\n",
    "df88 = pd.read_csv(\"file_to_save_cpu812layer_pruned90-none.csv\")\n",
    "df24 = df44[df44['Model Name']=='12layer_pruned90-none']\n",
    "df28 = df88[df88['Model Name']=='12layer_pruned90-none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28c120d0-3f2f-4421-94e7-101ad16645f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5.000000\n",
       "mean     0.024715\n",
       "std      0.000162\n",
       "min      0.024557\n",
       "25%      0.024619\n",
       "50%      0.024622\n",
       "75%      0.024866\n",
       "max      0.024914\n",
       "Name: Time per data point, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average time per data point\n",
    "\n",
    "df24['Time per data point'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc80543-142e-46e3-a388-0a1a3986132a",
   "metadata": {},
   "source": [
    "The average time per data point is 0.024715 seconds. A pdf with on average ~157 pages, and ~387 data points per page, will take ~25 mins, for 4 cores of CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4cd18dbb-801d-41d6-a2f5-e72bd7819876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5.000000\n",
       "mean     0.011791\n",
       "std      0.000056\n",
       "min      0.011749\n",
       "25%      0.011749\n",
       "50%      0.011756\n",
       "75%      0.011841\n",
       "max      0.011862\n",
       "Name: Time per data point, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average time per data point\n",
    "\n",
    "df28['Time per data point'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0350b9b-a14d-433d-bb28-64181d50f6ca",
   "metadata": {},
   "source": [
    "The average time per data point is 0.011791 seconds. A pdf with on average ~157 pages, and ~387 data points per page, will take ~12 mins, for 8 cores of CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7482cd7a-b857-4918-be92-6cc6b2a961df",
   "metadata": {},
   "source": [
    "**Model_Name : teacher**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ccd381a-ea61-4bfb-8894-f0dc56a0f1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df444 = pd.read_csv(\"file_to_save_cpu4teacher.csv\")\n",
    "df888 = pd.read_csv(\"file_to_save_cpu8teacher.csv\")\n",
    "df34 = df444[df444['Model Name']=='teacher']\n",
    "df38 = df888[df888['Model Name']=='teacher']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5bdf7460-22d9-4ca4-8be6-010274c55e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5.000000\n",
       "mean     0.096243\n",
       "std      0.001691\n",
       "min      0.094675\n",
       "25%      0.094822\n",
       "50%      0.095612\n",
       "75%      0.097979\n",
       "max      0.098129\n",
       "Name: Time per data point, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average time per data point\n",
    "\n",
    "df34['Time per data point'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef418dda-ca3f-44bf-b517-e7ead381a130",
   "metadata": {},
   "source": [
    "The average time per data point is 0.096243 seconds. A pdf with on average ~157 pages, and ~387 data points per page, will take ~97 mins to execute, for 4 cores of CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69d7d0e7-bef0-467a-b45e-8ff6972c8bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5.000000\n",
       "mean     0.045240\n",
       "std      0.002098\n",
       "min      0.041899\n",
       "25%      0.044501\n",
       "50%      0.046121\n",
       "75%      0.046759\n",
       "max      0.046919\n",
       "Name: Time per data point, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average time per data point\n",
    "\n",
    "df38['Time per data point'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a59be4-32db-43a8-ac19-5100bb2707df",
   "metadata": {},
   "source": [
    "The average time per data point is 0.045240 seconds. A pdf with on average ~157 pages, and ~387 data points per page, will take ~45 mins to execute, for 8 cores of CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f81b892-daf4-4479-be8e-bdc73ca63a2f",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb1c12d-8aef-4d19-8815-07d5b91e6204",
   "metadata": {},
   "source": [
    "Based on our experiments, we can conclude that DeepSparse and SparseZoo are effective in optimizing and executing custom knowledge distillation models on the CPU. In this notebook, we focused on calculating the inference time for three pre-trained custom knowledge distillation models: '12layer_pruned80-none', '12layer_pruned90-none', and 'teacher'. By running our experiments using 4 and 8 cores of the CPU, we were able to evaluate the effect of core count on the inference time of these models.\n",
    "\n",
    "Our results showed that the inference time varied across the different models. Additionally, we observed that increasing the number of cores led to a decrease in inference time for all models, which is in line with our expectations.\n",
    "\n",
    "Our findings demonstrate the potential of DeepSparse and SparseZoo for optimizing custom knowledge distillation models and highlight the importance of considering the number of CPU cores when evaluating their performance. This information can be useful for researchers and practitioners working with custom knowledge distillation models and contributes to the ongoing efforts to develop efficient and effective deep learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
